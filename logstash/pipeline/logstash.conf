input {
   
      file {
        type => "ginskeleton_nginx_access"
        path => "/usr/share/wwwlogs/project2021/ginskeleton_nginx_access.log"
        start_position => "beginning"
        stat_interval => "3"
        #  这里不要提前将原始数据转换为 json ，否则后面坑死你
        # codec => json
       }

       file {
        type => "ginskeleton-api"
        path => "/usr/share/wwwlogs/project2021/ginskeleton-api.log"
        start_position => "beginning"
        stat_interval => "3"
        codec => json
       }

       file {
        type => "ginskeleton-backend"
        path => "/usr/share/wwwlogs/project2021/ginskeleton-backend.log"
        start_position => "beginning"
        stat_interval => "3"
        codec => json
       }

   # 匹配nginx错误日志
   file {
        type => "nginxerr"
        path => "/usr/share/wwwlogs/project2021/error.log"
        start_position => "beginning"
        stat_interval => "3"
        codec => plain
   } 

}

filter {
      if [type]  != "nginxerr"  {
        json{
            source => "message"
        }
      }
	if [type] == "ginskeleton_nginx_access" {
	   mutate { add_field => { "[@metadata][target_index]" => "logstash-ginskeleton_nginx_access" } }
	    geoip {
     		source => "remote_addr"
	        target => "geoip"
		    database => "/usr/share/logstash/db_geoLite2_city/GeoLite2-City.mmdb"
	    }
	}else if [type]=="nginxerr"{
     	   mutate { add_field => { "[@metadata][target_index]" => "logstash-nginxerr-%{+YYYY-MM-dd}" } }
    }else if [type] == "ginskeleton-backend" {
           mutate { add_field => { "[@metadata][target_index]" => "logstash-ginskeleton-backend-%{+YYYY-MM-dd}" } }
    }else if [type] == "ginskeleton-api" {
           mutate { add_field => { "[@metadata][target_index]" => "logstash-ginskeleton-api-%{+YYYY-MM-dd}" } }
    }else {
	    mutate { add_field => { "[@metadata][target_index]" => "logstash-unknowindex-%{+YYYY-MM-dd}" } }
	}

	# 将 created_at 字符串格式的日期时间转换为 es 能够识别的日期格式
    grok {
        match => ["message","%{TIMESTAMP_ISO8601:created_at}"]
    }
    date {
        match => [ "created_at","ISO8601" ]
        target => "created_at"
    }
     

      # 匹配nginx错误日志
   if [type]=="nginxerr" {      
      grok {
         match => [ "message" , "(?<created_at>%{YEAR}[./-]%{MONTHNUM2}[./-]%{MONTHDAY} %{TIME:time2}) \[%{WORD:errLevel}] %{DATA:errMsg}, client\: %{IP:clientIp}(, server\: %{IPORHOST:server})?(, request\: \"%{DATA:request}\")?(, upstream\: \"%{DATA:upstream}\")?(, host\: \"%{DATA:host}\")?"  ]
        }
     }

    #删除一些多余字段
    mutate {
        remove_field => [ "message","@version"]
        #convert => [ "[geoip][coordinates]", "float" ]
    }
}

output {
    # 调试期间可以打开，这样看 logstash 的日志就会把过程数据打印出来
    #  stdout { codec => rubydebug }

	# 官方说，这里每出现一个elasticsearch都是一个数据库客户端连接，建议用一个连接一次性输出多个日志内容
     elasticsearch  {
        # 172.21.0.13 表示  elasticsearch 服务器所在的ip，logstash 作为客户端需要向 es 服务器的端口提交数据
         hosts => ["http://172.21.0.13:9200"]
         index => "%{[@metadata][target_index]}"
     }
}
